## Analyzing Antonym Heads in Transformer Language Models

This project explores the presence of antonym heads in transformer-based language models. Antonym heads are attention heads that map words to their semantic opposites (e.g., "hot" ➔ "cold", "true" ➔ "false").

Full writeup here: https://www.lesswrong.com/posts/XXK2T4EcbHRkRTBce/antonym-heads-predict-semantic-opposites-in-language-models
